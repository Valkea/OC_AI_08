# @package _global_

_version: 2  # An internal value that indicates a version of the config schema. This value is used by
# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.
# Please do not change it manually.


task: semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`.


policy_model:
  # Settings for Policy Model that searches augmentation policies.

  task_factor: 0.1
  # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations
  # from transforming images of a particular class to another class.

  gp_factor: 10
  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in
  # `Improved Training of Wasserstein GANs`.

  temperature: 0.05
  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from
  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of
  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors
  # of Faster AutoAugment used 0.05 as a default value for `temperature`.

  num_sub_policies: 25
  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment
  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger
  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on
  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search
  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.

  num_chunks: 4
  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it
  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff
  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to
  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching
  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations
  # to each image separately.

  operation_count: 4
  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`
  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of
  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search
  # and increase the searching time.

semantic_segmentation_model:
# Settings for Semantic Segmentation Model that is used for two purposes:
# 1. As a model that performs semantic segmentation of input images.
# 2. As a Discriminator for Policy Model.

  num_classes: 8
# The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with
# the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1.

  architecture: Unet
# The architecture of Semantic Segmentation Model. AutoAlbument uses models from
# https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available
# models - https://github.com/qubvel/segmentation_models.pytorch#models-.

  encoder_architecture: resnet18
# The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to
# get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders-

  pretrained: true
# Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained
# weights or use randomly initialized weights.
# - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly
#   initialized weights.
# - In the case of string the value should specify the name of the weights. For the list of available weights please
#   refer to https://github.com/qubvel/segmentation_models.pytorch#encoders-


data:
  dataset:
    _target_: dataset.SearchDataset
  # Class for instantiating a PyTorch dataset.
  
    num_classes: 8
# The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with
# the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1.

  dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 32
    shuffle: true
    num_workers: 8
    pin_memory: true
    drop_last: true
  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.

optim:
  main:
    _target_: torch.optim.Adam
    lr: 1e-3
    betas: [0, 0.999]
  # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model

  policy:
    _target_: torch.optim.Adam
    lr: 1e-3
    betas: [0, 0.999]
  # Optimizer configuration for Policy Model

seed: 0 # Random seed. If the value is not null, it will be passed to `seed_everything` -
# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything

hydra:
  run:
    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains
    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more
    # information - https://hydra.cc/docs/configure_hydra/workdir.

trainer:
  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at
  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.
  max_epochs: 50
  # Number of epochs to search for augmentation parameters.
  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs

  benchmark: true
  # If true enables cudnn.benchmark.
  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark

  gpus: 0
  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.
  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus
